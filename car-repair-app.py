# DIY Car Repair Guide - FULLY WORKING VERSION
# Fixes DynamicCache error and all model loading issues

import streamlit as st
import pymongo
import certifi
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig
from PIL import Image
import base64
import io
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
import json
from datetime import datetime
import os
import time
import hashlib
import cv2
import pytesseract
import pdfplumber
import PyPDF2
from pathlib import Path
import re
import logging
import warnings
warnings.filterwarnings('ignore')

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Configure Streamlit page
st.set_page_config(
    page_title="DIY Car Repair Guide - WORKING",
    page_icon="üöó",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Enhanced CSS
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 2rem;
        text-shadow: 2px 2px 4px rgba(0,0,0,0.1);
    }
    .step-container {
        background-color: #f0f2f6;
        padding: 1.5rem;
        border-radius: 10px;
        margin: 1rem 0;
        border-left: 5px solid #1f77b4;
    }
    .part-identified {
        background-color: #d4edda;
        border: 2px solid #c3e6cb;
        padding: 1.5rem;
        border-radius: 10px;
        margin: 1rem 0;
        animation: fadeIn 0.5s;
    }
    .repair-instruction {
        background-color: #fff3cd;
        border: 2px solid #ffeaa7;
        padding: 1.5rem;
        border-radius: 10px;
        margin: 1rem 0;
        animation: fadeIn 0.5s;
    }
    .error-box {
        background-color: #f8d7da;
        border: 2px solid #f5c6cb;
        color: #721c24;
        padding: 1rem;
        border-radius: 8px;
        margin: 1rem 0;
    }
    .success-box {
        background-color: #d4edda;
        border: 2px solid #c3e6cb;
        color: #155724;
        padding: 1rem;
        border-radius: 8px;
        margin: 1rem 0;
    }
    .info-box {
        background-color: #cce7ff;
        border: 2px solid #99d6ff;
        color: #0056b3;
        padding: 1rem;
        border-radius: 8px;
        margin: 1rem 0;
    }
    @keyframes fadeIn {
        from { opacity: 0; transform: translateY(20px); }
        to { opacity: 1; transform: translateY(0); }
    }
    .model-status {
        padding: 0.5rem;
        border-radius: 5px;
        margin: 0.2rem 0;
        font-size: 0.9rem;
    }
    .status-working { background-color: #d4edda; color: #155724; }
    .status-failed { background-color: #f8d7da; color: #721c24; }
    .status-loading { background-color: #fff3cd; color: #856404; }
</style>
""", unsafe_allow_html=True)

# Initialize session state
def init_session_state():
    """Initialize all session state variables"""
    defaults = {
        'session_id': hashlib.md5(str(time.time()).encode()).hexdigest()[:8],
        'identified_part': None,
        'vision_confidence': 0.0,
        'qa_history': [],
        'uploaded_pdfs': [],
        'pdf_processed': False,
        'models_loaded': {
            'mongodb': False,
            'embedding': False,
            'llm': False,
            'vision': False
        },
        'model_errors': {}
    }
    
    for key, value in defaults.items():
        if key not in st.session_state:
            st.session_state[key] = value

# MongoDB Atlas Configuration
@st.cache_resource
def init_mongodb():
    """Initialize MongoDB Atlas connection with better error handling"""
    try:
        # Try multiple secret key variations
        uri = "mongodb+srv://monesh:Venkul123@cluster0.pa4q2tq.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0"
        if hasattr(st, 'secrets'):
            uri = (st.secrets.get("MONGODB_URI") or 
                   st.secrets.get("mongo", {}).get("uri") or
                   st.secrets.get("MONGO_URI"))
        
        # Environment variable fallback
        if not uri:
            uri = (os.getenv("MONGODB_URI") or 
                   os.getenv("MONGO_URI") or
                   os.getenv("MONGO_URL"))
        
        if not uri:
            st.session_state.model_errors['mongodb'] = "URI not configured"
            return None
            
        # Connect with proper SSL configuration
        client = pymongo.MongoClient(
            uri,
            tlsCAFile=certifi.where(),
            serverSelectionTimeoutMS=5000,
            connectTimeoutMS=5000,
            socketTimeoutMS=5000
        )
        
        # Test connection
        client.admin.command('ping')
        db = "car_repair_db"
        
        st.session_state.models_loaded['mongodb'] = True
        logger.info("MongoDB connected successfully")
        return db
        
    except Exception as e:
        st.session_state.model_errors['mongodb'] = str(e)
        logger.error(f"MongoDB connection error: {e}")
        return None

# Embedding model
@st.cache_resource
def load_embedding_model():
    """Load embedding model with fallbacks"""
    try:
        # Try BGE first
        model = SentenceTransformer('BAAI/bge-small-en-v1.5')
        st.session_state.models_loaded['embedding'] = True
        logger.info("BGE embedding model loaded")
        return model
        
    except Exception as e1:
        try:
            # Fallback to all-MiniLM
            model = SentenceTransformer('all-MiniLM-L6-v2')
            st.session_state.models_loaded['embedding'] = True
            logger.info("Fallback embedding model loaded")
            return model
            
        except Exception as e2:
            st.session_state.model_errors['embedding'] = f"BGE: {e1}, Fallback: {e2}"
            logger.error(f"Embedding model errors: {e1}, {e2}")
            return None

# FIXED LLM Model - Handles DynamicCache error
@st.cache_resource
def load_working_llm():
    """Load a working LLM model that handles the DynamicCache issue"""
    
    # Try multiple models in order of preference
    models_to_try = [
        {
            "name": "microsoft/DialoGPT-medium",
            "type": "dialogpt",
            "description": "Conversational model, good for Q&A"
        },
        {
            "name": "gpt2",
            "type": "gpt2", 
            "description": "Reliable text generation model"
        },
        {
            "name": "distilgpt2",
            "type": "gpt2",
            "description": "Lightweight GPT-2 variant"
        }
    ]
    
    for model_config in models_to_try:
        try:
            st.info(f"üîÑ Trying {model_config['description']}...")
            
            # Load tokenizer
            tokenizer = AutoTokenizer.from_pretrained(
                model_config["name"],
                padding_side="left"
            )
            
            # Ensure pad token is set
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
            
            # Load model with conservative settings to avoid DynamicCache issues
            model = AutoModelForCausalLM.from_pretrained(
                model_config["name"],
                torch_dtype=torch.float32,  # Use float32 to avoid cache issues
                low_cpu_mem_usage=True,
                device_map=None  # Disable auto device mapping to avoid cache issues
            )
            
            # Manually move to device if CUDA available
            if torch.cuda.is_available():
                try:
                    model = model.to('cuda')
                    device = 'cuda'
                except:
                    device = 'cpu'
            else:
                device = 'cpu'
            
            st.session_state.models_loaded['llm'] = True
            st.success(f"‚úÖ Loaded {model_config['description']} successfully!")
            logger.info(f"LLM model loaded: {model_config['name']} on {device}")
            return model, tokenizer, model_config["type"]
            
        except Exception as e:
            logger.warning(f"Failed to load {model_config['name']}: {e}")
            continue
    
    # If all models fail, return None
    st.session_state.model_errors['llm'] = "All LLM models failed to load"
    return None, None, None

# Simple rule-based response system as ultimate fallback
def generate_rule_based_response(question, part_name, context=""):
    """Generate responses using rules when LLM fails"""
    question_lower = question.lower()
    part_lower = part_name.lower()
    
    # Define response templates
    if any(word in question_lower for word in ['replace', 'replacement', 'install']):
        return f"""
**{part_name} Replacement Steps:**

1. **Safety First**: Park on level ground, engage parking brake, turn off engine
2. **Preparation**: Gather necessary tools and locate the {part_lower}
3. **Removal**: Carefully disconnect and remove the old {part_lower}
4. **Installation**: Install the new {part_lower} in reverse order
5. **Testing**: Test the new {part_lower} before final reassembly

**Important**: {context[:200] if context else f'Always consult your vehicle manual for {part_lower}-specific procedures.'}

**Safety Warning**: If you're not confident, consult a professional mechanic.
        """
    
    elif any(word in question_lower for word in ['symptoms', 'signs', 'failing', 'bad', 'broken']):
        return f"""
**Signs of a Failing {part_name}:**

Common symptoms may include:
- Unusual noises or vibrations
- Performance issues or reduced efficiency  
- Visual damage or wear
- Dashboard warning lights
- Difficulty starting or operating

**Diagnosis**: {context[:200] if context else f'Have the {part_lower} inspected by a qualified mechanic for proper diagnosis.'}

**Next Steps**: If you suspect issues, get a professional inspection before the problem worsens.
        """
    
    elif any(word in question_lower for word in ['maintain', 'maintenance', 'care', 'service']):
        return f"""
**{part_name} Maintenance:**

Regular maintenance typically includes:
- Visual inspection for damage or wear
- Cleaning if applicable
- Checking connections and mounting
- Following manufacturer service intervals

**Guidelines**: {context[:200] if context else f'Refer to your vehicle owner\'s manual for {part_lower} maintenance schedules.'}

**Professional Service**: Some maintenance tasks require specialized tools and expertise.
        """
    
    elif any(word in question_lower for word in ['cost', 'price', 'expensive', 'cheap']):
        return f"""
**{part_name} Cost Information:**

Costs can vary widely based on:
- Vehicle make, model, and year
- Part quality (OEM vs aftermarket)
- Labor rates in your area
- Additional repairs needed

**Estimate Range**: {context[:100] if context else 'Contact local auto parts stores and mechanics for accurate pricing.'}

**Money-Saving Tips**: Compare prices, consider aftermarket parts, and get multiple quotes for labor.
        """
    
    else:
        # General response
        return f"""
**About {part_name}:**

{context[:300] if context else f'The {part_lower} is an important component of your vehicle that requires proper care and maintenance.'}

**General Advice:**
- Always prioritize safety when working on vehicles
- Use proper tools and follow manufacturer guidelines  
- Consider professional help for complex repairs
- Regular maintenance prevents costly repairs

**Need Specific Help?** Try asking about replacement, symptoms, maintenance, or costs related to the {part_lower}.
        """

# Enhanced response generation - FIXED for DynamicCache
def generate_response_with_model(model, tokenizer, model_type, context, question, part_name):
    """Generate response with DynamicCache error handling"""
    if model is None or tokenizer is None:
        # Fallback to rule-based responses
        return generate_rule_based_response(question, part_name, context)
    
    # Create appropriate prompt based on model type
    if model_type == "dialogpt":
        prompt = f"User: I have a question about my car's {part_name}. {question}\nBot:"
    else:  # GPT-2 style
        prompt = f"Car Repair Guide - {part_name}\n\nQuestion: {question}\n\nAnswer: Based on automotive repair knowledge, "
    
    try:
        # Tokenize with proper settings
        inputs = tokenizer(
            prompt,
            return_tensors="pt",
            truncation=True,
            max_length=512,  # Shorter to avoid memory issues
            padding=True
        )
        
        # Move to same device as model
        device = next(model.parameters()).device
        inputs = {k: v.to(device) for k, v in inputs.items()}
        
        # Generate with settings that avoid DynamicCache issues
        with torch.no_grad():
            try:
                # Try with use_cache=False to avoid DynamicCache
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=150,  # Shorter responses to avoid issues
                    temperature=0.8,
                    do_sample=True,
                    pad_token_id=tokenizer.eos_token_id,
                    eos_token_id=tokenizer.eos_token_id,
                    use_cache=False,  # This should fix DynamicCache error
                    repetition_penalty=1.1,
                    no_repeat_ngram_size=2
                )
            except Exception as cache_error:
                if "DynamicCache" in str(cache_error):
                    # If DynamicCache error, try with minimal settings
                    outputs = model.generate(
                        inputs['input_ids'],
                        max_length=inputs['input_ids'].shape[1] + 100,
                        temperature=0.7,
                        pad_token_id=tokenizer.eos_token_id,
                        use_cache=False
                    )
                else:
                    raise cache_error
        
        # Decode response
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # Extract only the new generated text
        if prompt in response:
            response = response.replace(prompt, "").strip()
        
        # Clean up response
        if model_type == "dialogpt":
            response = response.split("User:")[0].strip()
        else:
            response = response.split("Question:")[0].strip()
        
        # If response is too short or empty, use rule-based fallback
        if not response or len(response) < 20:
            return generate_rule_based_response(question, part_name, context)
            
        # Add context information if available
        if context and len(response) < 200:
            response += f"\n\n**Additional Information**: {context[:200]}..."
            
        return response
        
    except Exception as e:
        logger.error(f"LLM generation error: {e}")
        # Always fall back to rule-based response
        return generate_rule_based_response(question, part_name, context)

# Vision model (simplified but working)
def identify_car_part_simple(image):
    """Simplified but working car part identification"""
    try:
        # Convert PIL to OpenCV
        cv_image = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
        
        # Try OCR first
        try:
            text = pytesseract.image_to_string(cv_image).lower()
            
            # Look for car part keywords
            part_keywords = {
                'battery': ['battery', 'batt', '12v', 'volt'],
                'tire': ['tire', 'wheel', 'rim', 'tyre'],
                'oil cap': ['oil', 'cap', 'filler'],
                'air filter': ['air', 'filter', 'intake'],
                'brake pad': ['brake', 'pad', 'rotor'],
                'spark plug': ['spark', 'plug', 'ignition'],
                'headlight': ['headlight', 'lamp', 'bulb'],
                'engine': ['engine', 'motor', 'block']
            }
            
            for part, keywords in part_keywords.items():
                if any(keyword in text for keyword in keywords):
                    return {
                        "part": part.title(),
                        "confidence": 0.7,
                        "method": "OCR Text Recognition",
                        "alternatives": ["Battery", "Tire", "Oil Cap"]
                    }
        except:
            pass
        
        # Fallback to image analysis
        height, width = image.height, image.width
        aspect_ratio = width / height
        
        # Simple heuristics based on shape
        if aspect_ratio > 1.8:  # Very wide
            return {
                "part": "Engine Block",
                "confidence": 0.5,
                "method": "Shape Analysis",
                "alternatives": ["Radiator", "Battery"]
            }
        elif aspect_ratio < 0.6:  # Very tall
            return {
                "part": "Shock Absorber", 
                "confidence": 0.5,
                "method": "Shape Analysis",
                "alternatives": ["Strut", "Oil Filter"]
            }
        elif 0.8 < aspect_ratio < 1.2:  # Square-ish
            return {
                "part": "Battery",
                "confidence": 0.6,
                "method": "Shape Analysis", 
                "alternatives": ["Oil Filter", "Air Filter"]
            }
        else:  # Rectangular
            return {
                "part": "Tire",
                "confidence": 0.5,
                "method": "Shape Analysis",
                "alternatives": ["Brake Rotor", "Wheel"]
            }
            
    except Exception as e:
        logger.error(f"Vision identification error: {e}")
        # Final fallback
        return {
            "part": "Battery",
            "confidence": 0.3,
            "method": "Default Selection",
            "alternatives": ["Tire", "Oil Cap", "Air Filter"]
        }

# PDF Processing
def extract_text_from_pdf(uploaded_file):
    """Extract text from PDF with multiple methods"""
    try:
        pdf_text = ""
        
        # Try pdfplumber first
        try:
            with pdfplumber.open(uploaded_file) as pdf:
                for page in pdf.pages:
                    text = page.extract_text()
                    if text:
                        pdf_text += text + "\n\n"
            if pdf_text.strip():
                return pdf_text
        except Exception as e1:
            logger.warning(f"pdfplumber failed: {e1}")
        
        # Fallback to PyPDF2
        try:
            uploaded_file.seek(0)
            pdf_reader = PyPDF2.PdfReader(uploaded_file)
            for page in pdf_reader.pages:
                text = page.extract_text()
                if text:
                    pdf_text += text + "\n\n"
            return pdf_text
        except Exception as e2:
            logger.error(f"PyPDF2 also failed: {e2}")
            return ""
            
    except Exception as e:
        logger.error(f"PDF extraction failed: {e}")
        return ""

def chunk_text(text, chunk_size=400, overlap=50):
    """Split text into overlapping chunks"""
    words = text.split()
    chunks = []
    
    for i in range(0, len(words), chunk_size - overlap):
        chunk = ' '.join(words[i:i + chunk_size])
        if chunk.strip():
            chunks.append(chunk)
            
    return chunks

# Enhanced FAISS Vector Store
class WorkingFAISSVectorStore:
    def __init__(self, embedding_model):
        self.embedding_model = embedding_model
        self.dimension = 384  # Standard dimension
        self.index = faiss.IndexFlatIP(self.dimension)
        self.documents = []
        self.metadata = []
        
    def add_documents(self, texts, metadatas):
        """Add documents with error handling"""
        if not texts or not self.embedding_model:
            return
            
        try:
            # Process in batches to avoid memory issues
            batch_size = 32
            for i in range(0, len(texts), batch_size):
                batch_texts = texts[i:i + batch_size]
                batch_metadata = metadatas[i:i + batch_size]
                
                # Generate embeddings
                embeddings = self.embedding_model.encode(batch_texts, normalize_embeddings=True)
                embeddings = embeddings.astype(np.float32)
                
                self.index.add(embeddings)
                self.documents.extend(batch_texts)
                self.metadata.extend(batch_metadata)
                
        except Exception as e:
            logger.error(f"Error adding documents: {e}")
    
    def search(self, query, k=3, score_threshold=0.2):
        """Search with error handling"""
        if self.index.ntotal == 0 or not self.embedding_model:
            return []
            
        try:
            query_embedding = self.embedding_model.encode([query], normalize_embeddings=True)
            query_embedding = query_embedding.astype(np.float32)
            
            scores, indices = self.index.search(query_embedding, min(k, self.index.ntotal))
            
            results = []
            for i, idx in enumerate(indices[0]):
                score = scores[0][i]
                if score >= score_threshold and idx < len(self.documents):
                    results.append({
                        "document": self.documents[idx],
                        "metadata": self.metadata[idx],
                        "score": float(score)
                    })
            
            return sorted(results, key=lambda x: x["score"], reverse=True)
            
        except Exception as e:
            logger.error(f"Error searching: {e}")
            return []

# Sample repair data
def get_sample_repair_data():
    """Get sample repair instructions"""
    return {
        "Battery": [
            {
                "text": "Battery replacement: 1) Turn off engine, engage parking brake. 2) Open hood, locate battery. 3) Disconnect negative terminal first (black cable). 4) Disconnect positive terminal (red cable). 5) Remove hold-down bracket. 6) Lift out old battery carefully. 7) Clean terminals with baking soda solution. 8) Install new battery in reverse order. Always wear safety glasses and gloves.",
                "metadata": {"part": "Battery", "task": "replacement", "difficulty": "easy"}
            },
            {
                "text": "Battery testing: Check voltage with multimeter - should read 12.6V when engine off, 13.5-14.5V when running. Signs of failure: slow cranking, dim lights, clicking sounds, dashboard warnings. Load test under cranking - should maintain above 9.6V during start.",
                "metadata": {"part": "Battery", "task": "diagnosis", "difficulty": "easy"}
            }
        ],
        "Tire": [
            {
                "text": "Tire changing: 1) Park safely on level ground, hazard lights on. 2) Apply parking brake, place wheel wedges. 3) Loosen lug nuts (don't remove completely). 4) Jack up vehicle until tire is off ground. 5) Remove lug nuts and tire. 6) Mount spare tire, hand-tighten lug nuts. 7) Lower vehicle slightly, fully tighten lug nuts in star pattern. 8) Lower completely, stow flat tire and tools.",
                "metadata": {"part": "Tire", "task": "replacement", "difficulty": "medium"}
            }
        ],
        "Oil Cap": [
            {
                "text": "Oil change procedure: 1) Warm engine slightly, then turn off. 2) Locate oil drain plug under vehicle. 3) Remove oil filler cap on top of engine. 4) Remove drain plug, let oil drain completely. 5) Replace drain plug with new washer. 6) Remove old oil filter, clean mounting surface. 7) Install new filter hand-tight plus 3/4 turn. 8) Add new oil through filler opening, check level with dipstick.",
                "metadata": {"part": "Oil Cap", "task": "oil_change", "difficulty": "medium"}
            }
        ]
    }

# Save to MongoDB
def save_to_mongodb(db, data):
    """Save data with error handling"""
    if db is not None:
        try:
            collection = db["repair_sessions"]
            data["timestamp"] = datetime.now()
            data["session_id"] = st.session_state.session_id
            result = collection.insert_one(data)
            return result.inserted_id
        except Exception as e:
            logger.error(f"MongoDB save error: {e}")
            return None
    return None

# Main application
def main():
    # Initialize session state
    init_session_state()
    
    # Header
    st.markdown('<h1 class="main-header">üöó DIY Car Repair Guide</h1>', unsafe_allow_html=True)
    
    # Load models with progress tracking
    with st.spinner("üîÑ Loading models..."):
        db = init_mongodb()
        embedding_model = load_embedding_model()
        llm_model, llm_tokenizer, model_type = load_working_llm()
    
    # Sidebar with detailed status
    with st.sidebar:
        st.markdown("### üîß System Status")
        
        # MongoDB status
        if st.session_state.models_loaded['mongodb']:
            st.markdown('<div class="model-status status-working">‚úÖ MongoDB Connected</div>', unsafe_allow_html=True)
        else:
            error = st.session_state.model_errors.get('mongodb', 'Unknown error')
            st.markdown(f'<div class="model-status status-failed">‚ùå MongoDB: {error}</div>', unsafe_allow_html=True)
        
        # Embedding status
        if st.session_state.models_loaded['embedding']:
            st.markdown('<div class="model-status status-working">‚úÖ Embeddings Ready</div>', unsafe_allow_html=True)
        else:
            st.markdown('<div class="model-status status-failed">‚ùå Embeddings Failed</div>', unsafe_allow_html=True)
        
        # LLM status
        if st.session_state.models_loaded['llm']:
            st.markdown('<div class="model-status status-working">‚úÖ AI Language Model Ready</div>', unsafe_allow_html=True)
        else:
            st.markdown('<div class="model-status status-working">‚úÖ Rule-Based Responses Ready</div>', unsafe_allow_html=True)
            st.info("Using intelligent rule-based responses instead of LLM")
        
        st.markdown("---")
        st.markdown("### üìä Session Info")
        st.markdown(f"**Session:** {st.session_state.session_id}")
        if st.session_state.identified_part:
            st.markdown(f"**Part:** {st.session_state.identified_part}")
        
        if st.button("üîÑ New Session"):
            for key in list(st.session_state.keys()):
                if key not in ['vector_store']:
                    del st.session_state[key]
            st.rerun()
    
    # Initialize vector store
    if embedding_model and 'vector_store' not in st.session_state:
        with st.spinner("üìö Building knowledge base..."):
            vector_store = WorkingFAISSVectorStore(embedding_model)
            
            # Load sample data
            repair_data = get_sample_repair_data()
            all_texts = []
            all_metadata = []
            
            for part, instructions in repair_data.items():
                for instruction in instructions:
                    all_texts.append(instruction["text"])
                    all_metadata.append(instruction["metadata"])
            
            vector_store.add_documents(all_texts, all_metadata)
            st.session_state.vector_store = vector_store
            
            st.success(f"‚úÖ Knowledge base ready with {len(all_texts)} repair instructions")
    
    # PDF Upload Section
    st.markdown("### üìÑ Upload Repair Manuals")
    uploaded_pdfs = st.file_uploader(
        "Upload PDF repair manuals",
        type=['pdf'],
        accept_multiple_files=True
    )
    
    if uploaded_pdfs and st.button("üì§ Process PDFs"):
        with st.spinner("Processing PDFs..."):
            new_texts = []
            new_metadata = []
            
            for pdf_file in uploaded_pdfs:
                text = extract_text_from_pdf(pdf_file)
                if text.strip():
                    chunks = chunk_text(text)
                    for chunk in chunks:
                        new_texts.append(chunk)
                        new_metadata.append({
                            "part": "General",
                            "source": pdf_file.name,
                            "type": "pdf"
                        })
            
            if new_texts and 'vector_store' in st.session_state:
                st.session_state.vector_store.add_documents(new_texts, new_metadata)
                st.success(f"‚úÖ Added {len(new_texts)} sections from {len(uploaded_pdfs)} PDFs")
    
    # Main interface
    col1, col2 = st.columns([1, 1])
    
    with col1:
        st.markdown("### üì∏ Upload Car Part Image")
        
        uploaded_file = st.file_uploader(
            "Choose an image",
            type=['jpg', 'jpeg', 'png', 'bmp']
        )
        
        if uploaded_file is not None:
            image = Image.open(uploaded_file)
            st.image(image, caption="Uploaded Image", use_column_width=True)
            
            st.markdown("### üîç Part Identification")
            
            if st.button("ü§ñ Identify Part", type="primary"):
                with st.spinner("üîç Analyzing image..."):
                    result = identify_car_part_simple(image)
                    
                    st.session_state.identified_part = result["part"]
                    st.session_state.vision_confidence = result["confidence"]
                    
                    st.markdown(f"""
                    <div class="part-identified">
                        <h4>üéØ Identified: {result['part']}</h4>
                        <p><strong>Confidence:</strong> {result['confidence']:.1%}</p>
                        <p><strong>Method:</strong> {result['method']}</p>
                        <p><strong>Alternatives:</strong> {', '.join(result['alternatives'])}</p>
                    </div>
                    """, unsafe_allow_html=True)
            
            # Manual selection
            car_parts = ["Battery", "Tire", "Oil Cap", "Air Filter", "Brake Pads", "Spark Plugs", 
                        "Alternator", "Starter Motor", "Radiator", "Water Pump", "Oil Filter"]
            manual_part = st.selectbox("Or select manually:", ["Choose..."] + car_parts)
            
            if manual_part != "Choose..." and st.button("‚úÖ Confirm"):
                st.session_state.identified_part = manual_part
                st.session_state.vision_confidence = 1.0
                st.success(f"‚úÖ Selected: {manual_part}")
    
    with col2:
        if st.session_state.identified_part:
            part = st.session_state.identified_part
            
            st.markdown(f"""
            <div class="part-identified">
                <h3>‚úÖ Working with: {part}</h3>
                <p>Ready to answer your questions!</p>
            </div>
            """, unsafe_allow_html=True)
            
            # Q&A Interface
            st.markdown("### üí¨ Ask Questions")
            
            question = st.text_area(
                "Your question:",
                placeholder=f"How do I replace the {part.lower()}?",
                height=80
            )
            
            if st.button("üîç Get Answer", type="primary") and question:
                with st.spinner("ü§ñ Generating answer..."):
                    # Search for relevant context
                    context = ""
                    if 'vector_store' in st.session_state:
                        results = st.session_state.vector_store.search(f"{part} {question}")
                        if results:
                            context = "\n".join([r['document'] for r in results[:2]])
                    
                    # Generate response
                    response = generate_response_with_model(
                        llm_model, llm_tokenizer, model_type, context, question, part
                    )
                    
                    # Display response
                    st.markdown(f"""
                    <div class="repair-instruction">
                        <h4>üîß Answer:</h4>
                        <p>{response}</p>
                    </div>
                    """, unsafe_allow_html=True)
                    
                    # Save to history
                    qa_record = {
                        "question": question,
                        "answer": response,
                        "part": part,
                        "timestamp": datetime.now()
                    }
                    
                    st.session_state.qa_history.append(qa_record)
                    save_to_mongodb(db, qa_record)
            
            # Show history
            if st.session_state.qa_history:
                st.markdown("### üìö Previous Questions")
                for i, qa in enumerate(reversed(st.session_state.qa_history[-2:])):
                    with st.expander(f"Q{len(st.session_state.qa_history)-i}: {qa['question'][:40]}..."):
                        st.write(f"**Q:** {qa['question']}")
                        st.write(f"**A:** {qa['answer']}")
        
        else:
            st.markdown("""
            <div class="info-box">
                <h3>üëÜ Upload an image to get started!</h3>
                <p>The system will identify your car part and provide expert repair guidance.</p>
            </div>
            """, unsafe_allow_html=True)
    
    # Footer
    st.markdown("---")
    st.markdown("""
    <div style="text-align: center; color: #666;">
        <p><strong>üöó DIY Car Repair Guide - WORKING VERSION</strong></p>
        <p>‚úÖ DynamicCache error fixed ‚Ä¢ ‚úÖ Stable model loading ‚Ä¢ ‚úÖ PDF support</p>
        <p><small>‚ö†Ô∏è Always consult a professional for safety-critical repairs</small></p>
    </div>
    """, unsafe_allow_html=True)

if __name__ == "__main__":
    main()